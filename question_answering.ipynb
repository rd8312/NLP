{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bb5269-9235-40b1-800a-2a6b03bc628a",
   "metadata": {},
   "source": [
    "# ‰ª•TransformersÂ•ó‰ª∂ÂØ¶‰ΩúÂïèÁ≠î(Question Answering)ÂäüËÉΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caef63b5-5e4d-4b42-acd2-a259c511387c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import package\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4d69e4-8a14-4a57-ac0b-102f72d0cf7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "(‚Ä¶)distilled-squad/resolve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 473/473 [00:00<00:00, 188kB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261M/261M [00:07<00:00, 33.1MB/s]\n",
      "(‚Ä¶)squad/resolve/main/tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.0/29.0 [00:00<00:00, 9.52kB/s]\n",
      "(‚Ä¶)d-distilled-squad/resolve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213k/213k [00:00<00:00, 540kB/s]\n",
      "(‚Ä¶)tilled-squad/resolve/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436k/436k [00:00<00:00, 746kB/s]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a2a522-2f7b-47ef-954f-735cdcf69777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "context = r\"Extractive Question Answering is the task of extracting an answer \" + \\\n",
    "\"from a text given a question. An example of a question answering \" + \\\n",
    "\"dataset is the SQuAD dataset, which is entirely based on that task. \" + \\\n",
    "\"If you would like to fine-tune a model on a SQuAD task, you may \" + \\\n",
    "\"leverage the examples/question-answering/run_squad.py script.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9aae3b4-12b7-4e38-a4fa-e6553fe49d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'the task of extracting an answer from a text given a question', score: 0.6226 , start: 33, end: 94\n",
      "\n",
      "Answer: 'SQuAD dataset', score: 0.5053 , start: 146, end: 159\n"
     ]
    }
   ],
   "source": [
    "# test 2 sample\n",
    "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}\",\n",
    "        f\", start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "result = nlp(question=\"What is a good example of a question answering dataset?\", \n",
    "             context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}\",\n",
    "        f\", start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3133df-fe4e-44eb-ac6b-51e74053a76c",
   "metadata": {},
   "source": [
    "# ÁµêÂêàTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7319c2ae-11e4-4552-824d-a8031d46223c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(‚Ä¶)squad/resolve/main/tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.0/28.0 [00:00<00:00, 26.9kB/s]\n",
      "(‚Ä¶)finetuned-squad/resolve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [00:00<00:00, 224kB/s]\n",
      "(‚Ä¶)g-finetuned-squad/resolve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 682kB/s]\n",
      "(‚Ä¶)etuned-squad/resolve/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 1.52MB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:28<00:00, 47.6MB/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# ÁµêÂêàÂàÜË©ûÂô®(Tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abb5702-647e-40d9-bfb7-ae4c936aac29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ë®ìÁ∑¥Ë≥áÊñô\n",
    "text = r\"\"\"\n",
    "ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f7f65e-619e-4f2a-b3a7-f9d53109994d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÂïèÈ°å\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ü§ó Transformers?\",\n",
    "    \"What does ü§ó Transformers provide?\",\n",
    "    \"ü§ó Transformers provides interoperability between which frameworks?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d017b2b7-478f-4bfa-8eac-a4cf2b00ffe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  {'input_ids': tensor([[  101,  2129,  2116,  3653, 23654,  2098,  4275,  2024,  2800,  1999,\n",
      "           100, 19081,  1029,   102,   100, 19081,  1006,  3839,  2124,  2004,\n",
      "          1052, 22123,  2953,  2818,  1011, 19081,  1998,  1052, 22123,  2953,\n",
      "          2818,  1011,  3653, 23654,  2098,  1011, 14324,  1007,  3640,  2236,\n",
      "          1011,  3800,  4294,  2015,  1006, 14324,  1010, 14246,  2102,  1011,\n",
      "          1016,  1010, 23455,  1010, 28712,  2213,  1010,  4487, 16643, 23373,\n",
      "          1010, 28712,  7159,  1529,  1007,  2005,  3019,  2653,  4824,  1006,\n",
      "         17953,  2226,  1007,  1998,  3019,  2653,  4245,  1006, 17953,  2290,\n",
      "          1007,  2007,  2058,  3590,  1009,  3653, 23654,  2098,  4275,  1999,\n",
      "          2531,  1009,  4155,  1998,  2784,  6970, 25918,  8010,  2090, 23435,\n",
      "         12314,  1016,  1012,  1014,  1998,  1052, 22123,  2953,  2818,  1012,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input ids:  [101, 2129, 2116, 3653, 23654, 2098, 4275, 2024, 2800, 1999, 100, 19081, 1029, 102, 100, 19081, 1006, 3839, 2124, 2004, 1052, 22123, 2953, 2818, 1011, 19081, 1998, 1052, 22123, 2953, 2818, 1011, 3653, 23654, 2098, 1011, 14324, 1007, 3640, 2236, 1011, 3800, 4294, 2015, 1006, 14324, 1010, 14246, 2102, 1011, 1016, 1010, 23455, 1010, 28712, 2213, 1010, 4487, 16643, 23373, 1010, 28712, 7159, 1529, 1007, 2005, 3019, 2653, 4824, 1006, 17953, 2226, 1007, 1998, 3019, 2653, 4245, 1006, 17953, 2290, 1007, 2007, 2058, 3590, 1009, 3653, 23654, 2098, 4275, 1999, 2531, 1009, 4155, 1998, 2784, 6970, 25918, 8010, 2090, 23435, 12314, 1016, 1012, 1014, 1998, 1052, 22123, 2953, 2818, 1012, 102]\n",
      "Question: How many pretrained models are available in ü§ó Transformers?\n",
      "Answer: over 32 +\n",
      "inputs:  {'input_ids': tensor([[  101,  2054,  2515,   100, 19081,  3073,  1029,   102,   100, 19081,\n",
      "          1006,  3839,  2124,  2004,  1052, 22123,  2953,  2818,  1011, 19081,\n",
      "          1998,  1052, 22123,  2953,  2818,  1011,  3653, 23654,  2098,  1011,\n",
      "         14324,  1007,  3640,  2236,  1011,  3800,  4294,  2015,  1006, 14324,\n",
      "          1010, 14246,  2102,  1011,  1016,  1010, 23455,  1010, 28712,  2213,\n",
      "          1010,  4487, 16643, 23373,  1010, 28712,  7159,  1529,  1007,  2005,\n",
      "          3019,  2653,  4824,  1006, 17953,  2226,  1007,  1998,  3019,  2653,\n",
      "          4245,  1006, 17953,  2290,  1007,  2007,  2058,  3590,  1009,  3653,\n",
      "         23654,  2098,  4275,  1999,  2531,  1009,  4155,  1998,  2784,  6970,\n",
      "         25918,  8010,  2090, 23435, 12314,  1016,  1012,  1014,  1998,  1052,\n",
      "         22123,  2953,  2818,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input ids:  [101, 2054, 2515, 100, 19081, 3073, 1029, 102, 100, 19081, 1006, 3839, 2124, 2004, 1052, 22123, 2953, 2818, 1011, 19081, 1998, 1052, 22123, 2953, 2818, 1011, 3653, 23654, 2098, 1011, 14324, 1007, 3640, 2236, 1011, 3800, 4294, 2015, 1006, 14324, 1010, 14246, 2102, 1011, 1016, 1010, 23455, 1010, 28712, 2213, 1010, 4487, 16643, 23373, 1010, 28712, 7159, 1529, 1007, 2005, 3019, 2653, 4824, 1006, 17953, 2226, 1007, 1998, 3019, 2653, 4245, 1006, 17953, 2290, 1007, 2007, 2058, 3590, 1009, 3653, 23654, 2098, 4275, 1999, 2531, 1009, 4155, 1998, 2784, 6970, 25918, 8010, 2090, 23435, 12314, 1016, 1012, 1014, 1998, 1052, 22123, 2953, 2818, 1012, 102]\n",
      "Question: What does ü§ó Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "inputs:  {'input_ids': tensor([[  101,   100, 19081,  3640,  6970, 25918,  8010,  2090,  2029,  7705,\n",
      "          2015,  1029,   102,   100, 19081,  1006,  3839,  2124,  2004,  1052,\n",
      "         22123,  2953,  2818,  1011, 19081,  1998,  1052, 22123,  2953,  2818,\n",
      "          1011,  3653, 23654,  2098,  1011, 14324,  1007,  3640,  2236,  1011,\n",
      "          3800,  4294,  2015,  1006, 14324,  1010, 14246,  2102,  1011,  1016,\n",
      "          1010, 23455,  1010, 28712,  2213,  1010,  4487, 16643, 23373,  1010,\n",
      "         28712,  7159,  1529,  1007,  2005,  3019,  2653,  4824,  1006, 17953,\n",
      "          2226,  1007,  1998,  3019,  2653,  4245,  1006, 17953,  2290,  1007,\n",
      "          2007,  2058,  3590,  1009,  3653, 23654,  2098,  4275,  1999,  2531,\n",
      "          1009,  4155,  1998,  2784,  6970, 25918,  8010,  2090, 23435, 12314,\n",
      "          1016,  1012,  1014,  1998,  1052, 22123,  2953,  2818,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "input ids:  [101, 100, 19081, 3640, 6970, 25918, 8010, 2090, 2029, 7705, 2015, 1029, 102, 100, 19081, 1006, 3839, 2124, 2004, 1052, 22123, 2953, 2818, 1011, 19081, 1998, 1052, 22123, 2953, 2818, 1011, 3653, 23654, 2098, 1011, 14324, 1007, 3640, 2236, 1011, 3800, 4294, 2015, 1006, 14324, 1010, 14246, 2102, 1011, 1016, 1010, 23455, 1010, 28712, 2213, 1010, 4487, 16643, 23373, 1010, 28712, 7159, 1529, 1007, 2005, 3019, 2653, 4824, 1006, 17953, 2226, 1007, 1998, 3019, 2653, 4245, 1006, 17953, 2290, 1007, 2007, 2058, 3590, 1009, 3653, 23654, 2098, 4275, 1999, 2531, 1009, 4155, 1998, 2784, 6970, 25918, 8010, 2090, 23435, 12314, 1016, 1012, 1014, 1998, 1052, 22123, 2953, 2818, 1012, 102]\n",
      "Question: ü§ó Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "# Êé®Ê∏¨Á≠îÊ°à\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    print(\"inputs: \", inputs)\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    print(\"input ids: \", input_ids)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    # Get the most likely end of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "    )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed08e4-f4ae-4e88-a23a-01c16029a40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
