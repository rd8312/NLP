{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a066134a-2a05-4cde-975f-f8509f9ee39d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ÊïàËÉΩË™øÊ†°(Fine Tuning)‰ΩúÊ≥ï\n",
    "- https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb607d1-4025-498b-900b-739efb6c78a9",
   "metadata": {},
   "source": [
    "# ÂèÉÊï∏Ë®≠ÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f1344c-dc98-438b-bc7e-8edcd176a81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37452e1f-0071-4b9a-bd81-051e9826ab54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ÊåáÂÆö‰ªªÂãôÁÇ∫ cola\n",
    "task = \"cola\"\n",
    "# È†êÂÖàË®ìÁ∑¥Ê®°Âûã\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# ÊâπÈáè\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b182f322-0350-426c-a6b9-39cf084014a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\a4022\\AppData\\Local\\Temp\\ipykernel_7080\\2234095913.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "# ËºâÂÖ•Ë≥áÊñôÈõÜ\n",
    "dataset = datasets.load_dataset(\"glue\", actual_task)\n",
    "# ËºâÂÖ•ÊïàËÉΩË°°ÈáèÊåáÊ®ô\n",
    "metric = datasets.load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07be8647-9be5-4d5a-9f50-9acb9c8212a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cola'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CoLA(The Corpus of Linguistic Acceptability): Âà§Êñ∑Âè•Â≠êÊòØÂê¶Âêà‰πéË™ûÊ≥ï\n",
    "actual_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f129b7-6fa7-43a6-b1af-161c9e1ddf88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f901e9e-4f42-4c0f-91c9-81cc6463ed32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c056e67d-460b-4629-af10-2fdbbff5d8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\datasets\\dataset_dict.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?datasets.dataset_dict.DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b35fe4-b58b-43ac-8ae5-7995ad23f75f",
   "metadata": {},
   "source": [
    "### dataset Ë≥áÊñôÂûãÊÖãÁÇ∫ `DatasetDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "100d61f1-ead0-475e-b4f8-f91510ea0ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# È°ØÁ§∫Á¨¨‰∏ÄÁ≠ÜË≥áÊñô\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f05854-b1e3-4f6e-94d1-83e360853359",
   "metadata": {},
   "source": [
    "# ÂÆöÁæ©Èö®Ê©üÊäΩÂèñÊï∏ÊìöÂáΩÊï∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a41ff8-212b-40c1-a6a2-8faa157cb77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Èö®Ê©üÊäΩÂèñË≥áÊñôÂáΩÊï∏\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2860a309-b7dd-4248-a932-16b3dd7e426c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'll fix you a drink.</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fred watered the plants flat.</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bill coughed his way out of the restaurant.</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We're dancing the night away.</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Herman hammered the metal flat.</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The critics laughed the play off the stage.</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The pond froze solid.</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bill rolled out of the room.</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The gardener watered the flowers flat.</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The gardener watered the flowers.</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bill broke the bathtub into pieces.</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bill broke the bathtub.</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>They drank the pub dry.</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>They drank the pub.</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The professor talked us into a stupor.</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The professor talked us.</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We yelled ourselves hoarse.</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>We yelled ourselves.</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>We yelled Harry hoarse.</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Harry coughed himself into a fit.</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Harry coughed himself.</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Harry coughed us into a fit.</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bill followed the road into the forest.</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We drove Highway 5 from SD to SF.</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fred tracked the leak to its source.</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  label  idx\n",
       "0   Our friends won't buy this analysis, let alone...      1    0\n",
       "1   One more pseudo generalization and I'm giving up.      1    1\n",
       "2    One more pseudo generalization or I'm giving up.      1    2\n",
       "3      The more we study verbs, the crazier they get.      1    3\n",
       "4           Day by day the facts are getting murkier.      1    4\n",
       "5                               I'll fix you a drink.      1    5\n",
       "6                       Fred watered the plants flat.      1    6\n",
       "7         Bill coughed his way out of the restaurant.      1    7\n",
       "8                       We're dancing the night away.      1    8\n",
       "9                     Herman hammered the metal flat.      1    9\n",
       "10        The critics laughed the play off the stage.      1   10\n",
       "11                              The pond froze solid.      1   11\n",
       "12                       Bill rolled out of the room.      1   12\n",
       "13             The gardener watered the flowers flat.      1   13\n",
       "14                  The gardener watered the flowers.      1   14\n",
       "15                Bill broke the bathtub into pieces.      1   15\n",
       "16                            Bill broke the bathtub.      1   16\n",
       "17                            They drank the pub dry.      1   17\n",
       "18                                They drank the pub.      0   18\n",
       "19             The professor talked us into a stupor.      1   19\n",
       "20                           The professor talked us.      0   20\n",
       "21                        We yelled ourselves hoarse.      1   21\n",
       "22                               We yelled ourselves.      0   22\n",
       "23                            We yelled Harry hoarse.      0   23\n",
       "24                  Harry coughed himself into a fit.      1   24\n",
       "25                             Harry coughed himself.      0   25\n",
       "26                       Harry coughed us into a fit.      0   26\n",
       "27            Bill followed the road into the forest.      1   27\n",
       "28                  We drove Highway 5 from SD to SF.      1   28\n",
       "29               Fred tracked the leak to its source.      1   29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset[\"train\"][:30])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ccbbd9-b6ba-481d-90b2-30d180faca78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be very clever.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>4741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are three Davids in my class.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The tree lost some branches.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The cat had haven eaten.</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pat was neither recommended for promotion nor under any illusions about what that meant.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>6995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've never known as strong a person as Louise.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>5471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alison poked the needle into the cloth.</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>2835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Carmen obtained Mary a spare part.</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>2748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>John offered, and Harry gave, Sally a Cadillac.</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stephen is believed to be easy to annoy Ben.</td>\n",
       "      <td>unacceptable</td>\n",
       "      <td>4628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182df8d-e5a6-4922-9325-386226f19bad",
   "metadata": {},
   "source": [
    "## È°ØÁ§∫ÊïàËÉΩË°°ÈáèÊåáÊ®ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ad8393-cb46-4b9d-bb9a-e5a4c3649bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy(Ê∫ñÁ¢∫Áéá), F1 score,Pearson Correlation(ÈóúËÅØÂ∫¶), Spearman Correlation, Matthew Correlation\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db47bd-1d58-4c17-9be9-a9a73e7b7745",
   "metadata": {},
   "source": [
    "## Áî¢ÁîüÂÖ©Á≠ÜÈö®Ê©ü‰∫ÇÊï∏ÔºåÊ∏¨Ë©¶ÊïàËÉΩË°°ÈáèÊåáÊ®ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a502c2-e040-44fb-b6ce-fa51ab289c80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': -0.03126526997403612}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ada8ee-61f7-49fa-942b-a16c1a3d50cc",
   "metadata": {},
   "source": [
    "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
    "\n",
    "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
    "- for MNLI (matched or mismatched): Accuracy\n",
    "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for QNLI: Accuracy\n",
    "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "- for RTE: Accuracy\n",
    "- for SST-2: Accuracy\n",
    "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "- for WNLI: Accuracy\n",
    "\n",
    "so the metric object only computes the one(s) needed for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee1253-0f25-46b8-9eea-e02e41c2b55a",
   "metadata": {},
   "source": [
    "# step3: ÂàÜË©û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dcfade2-85d4-4f99-85fd-77515d2f5150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ÂàÜË©û\n",
    "# model_checkpoint: \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe0198-6bab-4c6b-89df-ba6f0cbd3c21",
   "metadata": {},
   "source": [
    "## Ê∏¨Ë©¶ÂÖ©Á≠ÜË≥áÊñôÔºåÈÄ≤Ë°åÂàÜË©û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e940f22f-08c1-447a-9817-2e57240497ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d74ee1-2448-495f-b10e-d660cc8170de",
   "metadata": {},
   "source": [
    "## ÂÆöÁæ©‰ªªÂãôÁöÑË≥áÊñôÈõÜÊ¨Ñ‰Ωç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c121b97e-5455-4424-894c-924fbd6f75d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba577c-30fd-43d7-ae42-f3d4dadd7dc1",
   "metadata": {},
   "source": [
    "## Ê∏¨Ë©¶Á¨¨‰∏ÄÁ≠ÜË≥áÊñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65bc978c-3723-45e1-8c96-68f03c93791d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0541a9-a4fd-447f-8733-dd8a9e4c4846",
   "metadata": {},
   "source": [
    "## Ê∏¨Ë©¶ 5 Á≠ÜË≥áÊñôÂàÜË©û"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e34b6fca-13c1-4910-8ad2-1d1193a5efdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "\n",
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "134d4656-3768-4ecc-9bd4-72567a93f5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b05e3-b32c-4987-9eb1-f7daf95b90e2",
   "metadata": {},
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b2cc4b1-7ae0-4fbd-b968-aa09e19cf93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Â∞áÊâÄÊúâË≥áÊñôÈÄ≤Ë°åÂàÜË©û\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b8e02-c6ac-4bc8-9e09-dfcea0e8255e",
   "metadata": {},
   "source": [
    "# step4: ÊïàËÉΩÂæÆË™ø(Fine tuning)ÔºåÂÖàÂä†ËºâÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7874759e-2355-48a5-8255-7b3b03d93b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# ËºâÂÖ•È†êÂÖàË®ìÁ∑¥ÁöÑÊ®°Âûã\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606547-6649-4a9b-8359-09053ea9a550",
   "metadata": {},
   "source": [
    "## ÂÆöÁæ©Ë®ìÁ∑¥ÂèÉÊï∏ÔºåÂèØÂèÉÈñ± [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abb4df97-b384-4efb-808b-512854c21b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" \\\n",
    "                        if task == \"cola\" else \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4f000-03e0-426a-ae39-bace44769da1",
   "metadata": {},
   "source": [
    "## ÂÆöÁæ©ÊïàËÉΩË°°ÈáèÊåáÊ®ôË®àÁÆóÁöÑÂáΩÊï∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba71d5ce-340a-4c20-ad22-fa0dce933f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3d2cd-c918-47d7-8360-9c69a15046c1",
   "metadata": {},
   "source": [
    "# step5: ÂÆöÁæ©Ë®ìÁ∑¥ËÄÖ(Trainer)Áâ©‰ª∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fe8dde5-ca5e-4bbd-bbb6-af2f734378bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48db41db-6219-410a-a94b-e4c9c8ac6023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \\\n",
    "                 \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c050110-e368-4321-a735-06f0d702791b",
   "metadata": {},
   "source": [
    "## Ê®°ÂûãË®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b26e154-2c80-4b16-8d6f-da8a08cb6553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2675' max='2675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2675/2675 02:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.521700</td>\n",
       "      <td>0.462327</td>\n",
       "      <td>0.460087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>0.519836</td>\n",
       "      <td>0.478542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>0.585863</td>\n",
       "      <td>0.512349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.758358</td>\n",
       "      <td>0.539970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.125100</td>\n",
       "      <td>0.819873</td>\n",
       "      <td>0.532412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2675, training_loss=0.2703213001857294, metrics={'train_runtime': 136.0028, 'train_samples_per_second': 314.369, 'train_steps_per_second': 19.669, 'total_flos': 229437415353012.0, 'train_loss': 0.2703213001857294, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d8a6f-23c3-446d-9777-7979b130e2cf",
   "metadata": {},
   "source": [
    "# step8: Ê®°ÂûãË©ï‰º∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9293ecac-13a6-43b2-9ed6-5555e1dd6dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7583578824996948,\n",
       " 'eval_matthews_correlation': 0.5399695537530284,\n",
       " 'eval_runtime': 0.642,\n",
       " 'eval_samples_per_second': 1624.658,\n",
       " 'eval_steps_per_second': 102.807,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11998e4-30c5-423f-8f7c-b52a47cb796f",
   "metadata": {},
   "source": [
    "# step9: Ê®°ÂûãÂ≠òÊ™î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed6f9fee-60b1-4007-95ee-d560cc0aba22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model('./cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70d26ea4-5788-4d4b-8af7-601237ef720d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.0816252,  2.294584 ],\n",
       "       [-2.1686707,  2.3378236]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.017, 'test_samples_per_second': 117.455, 'test_steps_per_second': 58.727})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tokenized_texts.items()}\n",
    "\n",
    "texts = [\"Hello, this one sentence!\", \"And this sentence goes with it.\"]    \n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True)\n",
    "new_dataset = SimpleDataset(tokenized_texts)\n",
    "trainer.predict(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7faac4a8-ed68-4981-9ed4-28051fc5190a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.0677142,  2.164078 ],\n",
       "       [-2.6961114,  2.9182317]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.016, 'test_samples_per_second': 125.261, 'test_steps_per_second': 62.631})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = tokenizer([\"They drank the pub.\", \"The professor talked us into a stupor.\"]\n",
    "                            , padding=True, truncation=True)\n",
    "new_dataset = SimpleDataset(tokenized_texts)\n",
    "trainer.predict(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcfa507c-7947-4879-84a3-c50906f6764a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.6207142,  2.7085028],\n",
       "       [-2.2014954,  2.3939314]], dtype=float32), label_ids=None, metrics={'test_runtime': 0.0156, 'test_samples_per_second': 128.439, 'test_steps_per_second': 64.22})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = tokenizer([\"Hello there!\", \"This is another text\"]\n",
    "                            , padding=True, truncation=True)\n",
    "new_dataset = SimpleDataset(tokenized_texts)\n",
    "trainer.predict(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c061720-cf95-4652-bcb5-96bde3ae73c4",
   "metadata": {},
   "source": [
    "# step7: ÊïàËÉΩË™øÊï¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b7b2f-e925-4ae2-808c-911cfcd7fd1b",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "- The `Trainer` supports hyperparameter search using [optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/). For this last section you will need either of those libraries installed, just uncomment the line you want on the next cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab30347-2882-4c61-916a-0de7150f370c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d724bdcb-952a-42b7-8fda-0c575a5b945c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install optuna\n",
    "#! pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "572d3627-aabf-4674-ac33-104a36398121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a400f27a-1935-4125-b286-2d5f5af838fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a813d6c7-2690-498d-837b-982c91cd2d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:02:27,713] A new study created in memory with name: no-name-bc6df64a-97b3-409d-a0b3-61bf08ec5e40\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.533316</td>\n",
       "      <td>0.355453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.500787</td>\n",
       "      <td>0.431450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.504701</td>\n",
       "      <td>0.435431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:03:19,568] Trial 0 finished with value: 0.4354313415465737 and parameters: {'learning_rate': 1.0667576859345529e-05, 'num_train_epochs': 3, 'seed': 6, 'per_device_train_batch_size': 64}. Best is trial 0 with value: 0.4354313415465737.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4276' max='4276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4276/4276 03:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.533439</td>\n",
       "      <td>0.366596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.668247</td>\n",
       "      <td>0.430677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.959647</td>\n",
       "      <td>0.430553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>1.073776</td>\n",
       "      <td>0.464223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:06:30,159] Trial 1 finished with value: 0.46422323628946977 and parameters: {'learning_rate': 6.599744206500645e-05, 'num_train_epochs': 4, 'seed': 9, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 0.46422323628946977.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1072' max='1072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1072/1072 01:55, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512989</td>\n",
       "      <td>0.394343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.516761</td>\n",
       "      <td>0.432928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.527688</td>\n",
       "      <td>0.429462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.531088</td>\n",
       "      <td>0.447995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:08:26,920] Trial 2 finished with value: 0.44799494293545944 and parameters: {'learning_rate': 1.0926819029622011e-05, 'num_train_epochs': 4, 'seed': 11, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 0.46422323628946977.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5345' max='5345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5345/5345 04:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.529260</td>\n",
       "      <td>0.403460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.507549</td>\n",
       "      <td>0.442530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.612596</td>\n",
       "      <td>0.473412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.730784</td>\n",
       "      <td>0.498608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.764165</td>\n",
       "      <td>0.484539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:12:55,617] Trial 3 finished with value: 0.48453921534304883 and parameters: {'learning_rate': 6.288787932757869e-06, 'num_train_epochs': 5, 'seed': 40, 'per_device_train_batch_size': 8}. Best is trial 3 with value: 0.48453921534304883.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 02:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572184</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.525554</td>\n",
       "      <td>0.380985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.514579</td>\n",
       "      <td>0.412227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0.432808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.508384</td>\n",
       "      <td>0.433597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:15:11,489] Trial 4 finished with value: 0.4335972500415117 and parameters: {'learning_rate': 5.326254733649814e-06, 'num_train_epochs': 5, 'seed': 21, 'per_device_train_batch_size': 64}. Best is trial 3 with value: 0.48453921534304883.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2138' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2138/10690 01:31 < 06:05, 23.37 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.610807</td>\n",
       "      <td>0.418298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:16:44,082] Trial 5 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1340' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1340/1340 02:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.471578</td>\n",
       "      <td>0.429998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.464675</td>\n",
       "      <td>0.515768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.537470</td>\n",
       "      <td>0.506716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.614644</td>\n",
       "      <td>0.511795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.660160</td>\n",
       "      <td>0.512324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:18:45,860] Trial 6 finished with value: 0.5123240976315012 and parameters: {'learning_rate': 2.237532596962579e-05, 'num_train_epochs': 5, 'seed': 36, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.5123240976315012.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2138' max='2138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2138/2138 02:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.476089</td>\n",
       "      <td>0.469169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.642732</td>\n",
       "      <td>0.529477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:21:19,261] Trial 7 finished with value: 0.5294768861655004 and parameters: {'learning_rate': 4.05349397012981e-05, 'num_train_epochs': 2, 'seed': 3, 'per_device_train_batch_size': 8}. Best is trial 7 with value: 0.5294768861655004.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1069' max='1069' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1069/1069 00:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.544174</td>\n",
       "      <td>0.322108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:22:10,525] Trial 8 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a4022\\anaconda3\\envs\\gpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2138' max='2138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2138/2138 01:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.564417</td>\n",
       "      <td>0.388571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-05 01:23:54,620] Trial 9 pruned. \n"
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df7df1-6f00-4513-807a-a8eff778704f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
