{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "632ae4fd-54df-4393-a730-f7a817798bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15eb82f4-03ab-4ebc-9e16-e541a08338b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783f4e39-8857-47d6-bda5-b7396fef76fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequency\n",
    "# total nine words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2216ed69-5d12-46e0-8c17-44b85fff5dfa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__sklearn_clone__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_request_for_signature',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_default_requests',\n",
       " '_get_metadata_request',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_validate_data',\n",
       " '_validate_ngram_range',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names_out',\n",
       " 'get_metadata_routing',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'preprocessor',\n",
       " 'set_fit_request',\n",
       " 'set_params',\n",
       " 'set_transform_request',\n",
       " 'stop_words',\n",
       " 'stop_words_',\n",
       " 'strip_accents',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a6ef0a-ca08-47db-824b-7bf004a80ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary： ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "word = vectorizer.get_feature_names_out()\n",
    "print (\"Vocabulary：\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4616de-31eb-4aad-90d4-f74a998d7103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW=\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# BOW\n",
    "print (\"BOW=\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee795f3d-9d95-4f16-8bc0-4407a3064fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=\n",
      " [[0.     0.4698 0.5803 0.3841 0.     0.     0.3841 0.     0.3841]\n",
      " [0.     0.6876 0.     0.2811 0.     0.5386 0.2811 0.     0.2811]\n",
      " [0.5118 0.     0.     0.2671 0.5118 0.     0.2671 0.5118 0.2671]\n",
      " [0.     0.4698 0.5803 0.3841 0.     0.     0.3841 0.     0.3841]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF transform\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print (\"TF-IDF=\\n\", np.around(tfidf.toarray(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302b18a6-8f6f-402a-a1e5-5ae2acee0a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print (\"TF-IDF=\\n\", np.around(tfidf.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498e025a-7d8d-407a-8f59-9eda4193c530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.30777186787740496\n",
      "  (0, 1)\t0.6469256761082494\n",
      "  (0, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# compare similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print (cosine_similarity(tfidf[-1], tfidf[:-1], dense_output=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00b1ea-5758-4db6-b481-19721a3d926f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
